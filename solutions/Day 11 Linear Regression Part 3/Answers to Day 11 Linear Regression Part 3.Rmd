---
title: "Day 10 Linear Regression Part 3"
author: "Carey Kopeikin"
date: "1/9/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


#Linear Regression Day 3

What you will learn in this lesson:

* Interpreting residual plots
* What to do with data that is non linear


Upload the data set ```nonlinear.examples.csv``` and call it ```examples.data```

```{r}
examples.data <- read.csv("nonlinear.examples.csv")
```


Create a scatterplot with labels with Var1 as the explanatory variable and Var2 as the response variable. 

```{r}
plot(examples.data$Var2 ~examples.data$Var1,
     main = "Scatterplot of Var1 vs Var2",
     xlab = "Var1",
     ylab = "Var2",
     col = "blue",
     bg = "yellow",
     pch = 22,
     cex = 1.5)
```

Describe the shape, direction, and strength of the association:
*Your answer here.*

linear, negative and strong.

Is it appropriate to use correlation to talk about the relationship between these variables? Explain why or why not.

*Your answer here.*
Yes because the variables are both quantitative and they have a liner association. 


Find the correlation
```{r}
cor(examples.data$Var2, examples.data$Var1)
```


Create a linear model and print out the results.

```{r}
lin.mod.var2.var1 <- lm( Var2 ~ Var1, data = examples.data)
lin.mod.var2.var1
```

Make a scatterplot that includes the line of best fit.

```{r}
plot(examples.data$Var2 ~examples.data$Var1,
     main = "Scatterplot of Var1 vs Var2",
     xlab = "Var1",
     ylab = "Var2",
     col = "blue",
     bg = "yellow",
     pch = 22,
     cex = 1.5)

abline(lin.mod.var2.var1)
```

Find the predicted values and the residuals and add them to the data frame. 

```{r}
examples.data$Predictions.var2.var1 <- predict(lin.mod.var2.var1)
examples.data$Residuals.var2.var1 <- resid(lin.mod.var2.var1)

```


Make a graph of the predicted values and the residuals.

```{r}
plot(examples.data$Residuals.var2.var1 ~examples.data$Predictions.var2.var1,
     main = "Residual Plot of Var1 vs Var2",
     xlab = "Predictions",
     ylab = "Residuals",
     col = "blue",
     bg = "yellow",
     pch = 22,
     cex = 1.5)

abline(0,0,
       lwd = 3,
       col = "red")

```


Do you still think a linear model is appropriate? Why?

No, there is a distinct pattern in the residuals which tells us that we could improve on the model. In this case we would be better off guessing higher at our lower predictions, guessing lower at the middle range of predictions and guessing higher at our highest predictions. The pattern tells us that the original scatterplot was actually not linear. When I recreate it below I can now see that the relationship is actually curved (inverse logarithmic) even though originally I thought it was linear.


```{r}
plot(examples.data$Var2 ~examples.data$Var1,
     main = "Scatterplot of Var1 vs Var2",
     xlab = "Var1",
     ylab = "Var2",
     col = "blue",
     bg = "yellow",
     pch = 22,
     cex = 1.5)

abline(lin.mod.var2.var1,
       )
```

